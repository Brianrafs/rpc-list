## ğŸ“¦ Projeto: RemoteList RPC com PersistÃªncia e ConcorrÃªncia

Este projeto implementa um sistema distribuÃ­do em Go com chamadas RPC sÃ­ncronas para manipular mÃºltiplas listas de inteiros de forma concorrente e persistente.

### ğŸš€ Funcionalidades

* Adicionar, consultar, remover e obter tamanho de listas via RPC
* Suporte a mÃºltiplos clientes simultÃ¢neos
* PersistÃªncia com arquivos de log e snapshot
* RecuperaÃ§Ã£o automÃ¡tica de estado apÃ³s falhas
* ExclusÃ£o mÃºtua com `sync.Mutex` e `sync.RWMutex`

---

### ğŸ—‚ Estrutura do Projeto

```
rpc-list/
â”œâ”€â”€ main.go                  # Inicia o servidor RPC
â”œâ”€â”€ client/
â”‚   â””â”€â”€ client.go            # Cliente de exemplo
â””â”€â”€ server/
    â”œâ”€â”€ remote_list.go       # LÃ³gica principal do RemoteList
    â”œâ”€â”€ persistence.go       # PersistÃªncia (log/snapshot)
    â””â”€â”€ types.go             # Tipos auxiliares
```

---

### â–¶ï¸ Como Executar

#### 1. Iniciar o Servidor

```bash
go run main.go
```

Servidor escutarÃ¡ na porta `:1234`.

#### 2. Rodar o Cliente

```bash
go run client/client.go
```

VocÃª verÃ¡ saÃ­das como:

```
[Client 3] Append: Valor adicionado.
[Client 3] Size: 2
[Client 0] Append: Valor adicionado.
[Client 4] Append: Valor adicionado.
[Client 3] Get index 0: 30
[Client 1] Append: Valor adicionado.
[Client 4] Size: 5
[Client 0] Size: 5
[Client 2] Append: Valor adicionado.
[Client 2] Size: 4
[Client 0] Get index 0: 30
[Client 1] Size: 4
[Client 4] Get index 0: 30
[Client 3] Remove: 20
[Client 1] Get index 0: 30
[Client 2] Get index 0: 30
[Client 0] Remove: 10
[Client 4] Remove: 40
[Client 1] Remove: 0
[Client 2] Remove: 30
Todos os clientes terminaram.
```

---

### ğŸ“ PersistÃªncia

* `snapshot.json`: armazena o estado completo das listas periodicamente.
* `log.jsonl`: armazena todas as operaÃ§Ãµes (append/remove).

No reinÃ­cio do servidor, ele recupera o estado a partir do snapshot e aplica operaÃ§Ãµes pendentes do log.

---

### ğŸ§  DiscussÃ£o: LimitaÃ§Ãµes e Escalabilidade

* **ConsistÃªncia**: O sistema garante consistÃªncia por meio do uso de mecanismos de exclusÃ£o mÃºtua, utilizando mutexes especÃ­ficos para cada lista e um mutex global durante a criaÃ§Ã£o de snapshots. Dessa forma, evita-se condiÃ§Ãµes de corrida e inconsistÃªncias nos dados durante acessos simultÃ¢neos por mÃºltiplos clientes.

* **Escalabilidade**: A escalabilidade do sistema Ã© limitada, uma vez que se trata de uma arquitetura monolÃ­tica com persistÃªncia baseada em arquivos locais. Para permitir um crescimento mais eficiente, seria possÃ­vel adotar estratÃ©gias como particionamento das listas (sharding), utilizaÃ§Ã£o de sistemas de armazenamento distribuÃ­do (como etcd, Redis ou Cassandra) e balanceamento de carga entre instÃ¢ncias do serviÃ§o.

* **Disponibilidade**: Atualmente, o sistema possui um ponto Ãºnico de falha, jÃ¡ que depende de uma Ãºnica instÃ¢ncia do servidor. Para aumentar a disponibilidade e tolerÃ¢ncia a falhas, seria recomendada a implementaÃ§Ã£o de replicaÃ§Ã£o ativa/passiva, permitindo que instÃ¢ncias secundÃ¡rias assumam o controle em caso de falha da principal.

* **Falhas**: O sistema realiza a recuperaÃ§Ã£o do estado das listas utilizando snapshots periÃ³dicos e registros de log das operaÃ§Ãµes. Essa abordagem garante persistÃªncia mesmo apÃ³s falhas do servidor. No entanto, caso ocorra a perda do arquivo de log apÃ³s o Ãºltimo snapshot, existe a possibilidade de perda das operaÃ§Ãµes realizadas nesse intervalo. EstratÃ©gias de backup e replicaÃ§Ã£o poderiam mitigar esse risco.